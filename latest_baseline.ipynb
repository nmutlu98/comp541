{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Char}:\n",
       " '・': Unicode U+30FB (category Po: Punctuation, other)\n",
       " '･': Unicode U+FF65 (category Po: Punctuation, other)\n",
       " '*': ASCII/Unicode U+002A (category Po: Punctuation, other)\n",
       " '-': ASCII/Unicode U+002D (category Pd: Punctuation, dash)\n",
       " '一': Unicode U+4E00 (category Lo: Letter, other)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character codes for baseline punctuation split model\n",
    "ending_punc = [\n",
    "    '\\u0964',\n",
    "    '\\u061F',\n",
    "    '\\u002E',\n",
    "    '\\u3002',\n",
    "    '\\u0021',\n",
    "    '\\u06D4',\n",
    "    '\\u17D4',\n",
    "    '\\u003F',\n",
    "    '\\uFF61',\n",
    "    '\\uFF0E',\n",
    "    '\\u2026',\n",
    "]\n",
    "\n",
    "closing_punc = [\n",
    "    '\\u3011',\n",
    "    '\\u00BB',\n",
    "    '\\u201D',\n",
    "    '\\u300F',\n",
    "    '\\u2018',\n",
    "    '\\u0022',\n",
    "    '\\u300D',\n",
    "    '\\u201C',\n",
    "    '\\u0027',\n",
    "    '\\u2019',\n",
    "    '\\u0029'\n",
    "]\n",
    "\n",
    "list_set = [\n",
    "    '\\u30fb',\n",
    "    '\\uFF65',\n",
    "    '\\u002a', # asterisk\n",
    "    '\\u002d',\n",
    "    '\\u4e00' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rule for English\n",
    "struct PunctuationSpace\n",
    "end\n",
    "\n",
    "function (obj :: PunctuationSpace)(left_context, right_context)\n",
    "    left_context_regex = r\".*[?!.][.?!\\\")\\']*\"\n",
    "    # if the right context doesnt start with a number\n",
    "    if !occursin(right_context[1], \"012345678\")\n",
    "        # if the left context has an ending punctuation followed by a number of punctuations\n",
    "        if match(left_context_regex, left_context) != nothing\n",
    "            # the site is a candidate site\n",
    "            return true\n",
    "        end\n",
    "    end\n",
    "    return false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context_to_tensor (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "struct Vocabulary\n",
    "    itos\n",
    "    stoi\n",
    "    Vocabulary(itos=[\"<unk>\", \"<eos>\", \"<mos>\", \"<pad>\"], stoi = Dict(\"<unk>\" => 1, \"<eos>\" => 2, \"<mos>\" => 3, \"<pad>\" => 4)) = new(itos, stoi)\n",
    "end\n",
    "\n",
    "Base.length(v :: Vocabulary) = length(v.itos)\n",
    "\n",
    "function add_word_to_vocab(v :: Vocabulary, word)\n",
    "    if get(v.stoi, word, nothing) == nothing\n",
    "        v.stoi[word] = length(v) + 1\n",
    "        push!(v.itos, word)\n",
    "    end\n",
    "end\n",
    "# Goes overs the words in the file and records them to vocab\n",
    "function build_vocab(v :: Vocabulary, file_path)\n",
    "    open(file_path) do f\n",
    "        for line in readlines(f)\n",
    "            for word in split(line)\n",
    "                add_word_to_vocab(v, strip(word))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "# Returns the unique index of the word\n",
    "function embed_word(v :: Vocabulary, word)\n",
    "    return get(v.stoi, word, 3)\n",
    "end\n",
    "# Returns the word associated with the unique index\n",
    "function get_word(v :: Vocabulary, embedding)\n",
    "    return v.itos[embedding]\n",
    "end\n",
    "\n",
    "function detokenize(v :: Vocabulary, input_string)\n",
    "    input_string = replace(input_string, ' ' => \"\")\n",
    "    input_string = replace(input_string, \"\\u2581\" => ' ')\n",
    "    return input_string\n",
    "end\n",
    "# If out_type is int it uses the list of unique indices of the words in input string\n",
    "# Otherwise it returns the list of words in input string\n",
    "function encode(v :: Vocabulary, input_string; out_type=Int64)\n",
    "    if out_type == Int64\n",
    "        embedded_words = []\n",
    "        input_string = split(input_string)\n",
    "        for s in input_string\n",
    "            push!(embedded_words, embed_word(v, s))\n",
    "        end\n",
    "        return embedded_words\n",
    "    else\n",
    "        return split(input_string) \n",
    "    end\n",
    "end\n",
    "    \n",
    "# Returns the words for given list of unique indices\n",
    "function decode(v :: Vocabulary, input_array)\n",
    "    output = []\n",
    "    for i in input_array\n",
    "        push!(output, get_word(v, i))\n",
    "    end\n",
    "    return join(output, ' ')\n",
    "end\n",
    "\n",
    "function merge(v :: Vocabulary, sentence; technique = \"replace\")\n",
    "    if technique == \"replace\"\n",
    "        return replace(replace(sentence, ' ', \"\"), \"▁\" => \" \") \n",
    "    else\n",
    "        return decode(v, sentence)\n",
    "    end\n",
    "end\n",
    "\n",
    "# This method creates the embeddings of the context arrays \n",
    "function context_to_tensor(v :: Vocabulary, contexts)\n",
    "    con_arr = []\n",
    "    fact_arr = []\n",
    "    lab_arr = []\n",
    "    for (left, right, label) in contexts\n",
    "        tens = []\n",
    "        for l in split(left)\n",
    "            push!(tens, embed_word(v, l))\n",
    "        end\n",
    "        for r in split(right)\n",
    "            push!(tens, embed_word(v, r))\n",
    "        end\n",
    "        push!(con_arr, tens)\n",
    "        if label == \"<eos>\"\n",
    "            push!(lab_arr, 0)\n",
    "        else\n",
    "            push!(lab_arr, 1)\n",
    "        end\n",
    "    end\n",
    "    return (con_arr, lab_arr) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_test_file (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads the document and constructs left and right contexts \n",
    "function split_test_file(document, tokenizer, left_context_size, right_context_size)\n",
    "    document = encode(tokenizer, document, out_type=String)\n",
    "    left_contexts = []\n",
    "    right_contexts = []\n",
    "    if length(document) > 0\n",
    "        left_temp = [\"<pad>\" for i in collect(1 : left_context_size - 1)]\n",
    "        push!(left_temp, document[1])\n",
    "        right_temp = [x for x in document[collect(2 : (right_context_size) + 1)]]\n",
    "        \n",
    "        while length(right_temp) < right_context_size\n",
    "            push!(right_temp, \"<pad>\")\n",
    "        end\n",
    "        temp_index = right_context_size + 2\n",
    "    \n",
    "        for (index, word) in enumerate(document)\n",
    "            push!(left_contexts, join(left_temp, \" \"))\n",
    "            push!(right_contexts, join(right_temp, \" \"))\n",
    "            deleteat!(left_temp, 1)\n",
    "            push!(left_temp, right_temp[1])\n",
    "            deleteat!(right_temp, 1)\n",
    "            if temp_index < length(document)\n",
    "                push!(right_temp, document[temp_index])\n",
    "                temp_index += 1\n",
    "            else\n",
    "                push!(right_temp, \"<pad>\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return left_contexts, right_contexts\n",
    "end\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallel_evaluation (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function detokenize(input_string)\n",
    "    return replace(replace(input_string, ' ' => \"\"), '\\u2581' => ' ')\n",
    "end\n",
    "struct EvalModel\n",
    "    model\n",
    "    tokenizer\n",
    "    left_context_size\n",
    "    right_context_size\n",
    "    context_size\n",
    "    EvalModel(model) = new(model, model.tokenizer, model.left_context_size, model.right_context_size, model.left_context_size + model.right_context_size)\n",
    "end\n",
    "# Takes the content and extracts left and right contexts\n",
    "# Then detects candidate sites and divides them into batches \n",
    "# for model prediction\n",
    "function batchify(obj :: EvalModel, content, batch_size, candidates)\n",
    "    left_contexts, right_contexts = split_test_file(content, obj.tokenizer, obj.left_context_size, obj.right_context_size)\n",
    "    # lines include left and right contexts, whereas indices includes the indices of the candidate sites\n",
    "    # in the file text\n",
    "    if length(left_contexts) > 0\n",
    "        lines = []\n",
    "        indices = []\n",
    "        index = 1\n",
    "        for (left, right) in zip(left_contexts, right_contexts)\n",
    "            if candidates(detokenize(join(left, ' ')), detokenize(join(right, ' ')))   \n",
    "                push!(lines, (left, right, \"<eos>\"))\n",
    "                push!(indices, index)\n",
    "            end\n",
    "            index += 1\n",
    "        end\n",
    "        \n",
    "        data, _ = context_to_tensor(obj.tokenizer, lines)\n",
    "        \n",
    "        nbatch = div(size(data)[1], batch_size)\n",
    "        remainder = size(data)[1] % batch_size\n",
    "        \n",
    "        if remainder > 0\n",
    "            remaining_data = copy(data)[collect(nbatch * batch_size + 1 : nbatch * batch_size + remainder)]\n",
    "            remaining_indices = copy(indices)[collect(nbatch * batch_size + 1 : nbatch * batch_size + remainder)]\n",
    "        end\n",
    "        \n",
    "        data = data[collect(1:nbatch * batch_size)]\n",
    "        indices = indices[collect(1:nbatch * batch_size)]\n",
    "        \n",
    "        \n",
    "        data = reshape(data, :, batch_size)\n",
    "        indices = reshape(indices, :, batch_size)\n",
    "        \n",
    "        if remainder > 0\n",
    "            remaining_data = reshape(remaining_data, :, remainder)\n",
    "            remaining_indices = reshape(remaining_indices, :, remainder)\n",
    "        end\n",
    "        \n",
    "        batches = []\n",
    "        for i in collect(1 : size(data)[1])\n",
    "            context_batch = Array(data[i, :])'\n",
    "            index_batch = indices[i, :]\n",
    "            push!(batches, (context_batch, index_batch))\n",
    "        end\n",
    "        \n",
    "        if remainder > 0\n",
    "            push!(batches, (Array(remaining_data[1, :])', indices[1, :]))\n",
    "        \n",
    "        end\n",
    "        \n",
    "        return batches\n",
    "    else\n",
    "        return []\n",
    "    end\n",
    "end\n",
    "\n",
    "function parallel_evaluation(obj :: EvalModel, content, batch_size; candidates = nothing, min_sent_size = 3)\n",
    "    batches = batchify(obj, content, batch_size, candidates)\n",
    "    eos = []\n",
    "    for (contexts, indices) in batches\n",
    "        data = contexts\n",
    "        # Normally model does prediction at all positions here because batchify function\n",
    "        # returns all the candidate sites\n",
    "        for index in indices\n",
    "            push!(eos, index)\n",
    "        end\n",
    "    end\n",
    "    if length(eos) == 0\n",
    "        return strip(content)\n",
    "    else\n",
    "        #BAZI UNIQUE OLMAYAN ENTRYLER VAR BU PROBLEM\n",
    "        eos = unique(eos)\n",
    "        eos = sort(eos)\n",
    "        next_index = eos[1]\n",
    "        deleteat!(eos, 1)\n",
    "        this_content = encode(obj.tokenizer, content, out_type = String)\n",
    "        output = []\n",
    "        counter = 1\n",
    "        # for each candidate site a newline is added after it \n",
    "        for (index, word) in enumerate(this_content)\n",
    "            if counter == next_index\n",
    "                if !isempty(eos)\n",
    "                    next_index = eos[1]\n",
    "                    deleteat!(eos, 1)\n",
    "                end\n",
    "                push!(output, word)\n",
    "                push!(output, \"\\n\")\n",
    "                \n",
    "            else\n",
    "                push!(output, word)\n",
    "            end\n",
    "            counter += 1\n",
    "        end\n",
    "        \n",
    "        output = join(output, \" \")\n",
    "        output = replace(output, r\" \\n\" => \"\\n\")\n",
    "        \n",
    "        return output\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its given a tokenizer for detecting candidate sites a left context size and a right context size\n",
    "struct AlwaysSplitModel\n",
    "    tokenizer\n",
    "    left_context_size\n",
    "    right_context_size\n",
    "    AlwaysSplitModel(tokenizer, left_context_size, right_context_size) = new(tokenizer, left_context_size, right_context_size)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_model (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function for evaluating the model, creates the final segmented file \n",
    "function evaluate_model(eval_model, input_file, output_file, batch_size; candidates=nothing)\n",
    "    output = open(output_file, \"w\")\n",
    "    input = open(input_file, \"r\")\n",
    "    for line in readlines(input)\n",
    "        for batch_output in parallel_evaluation(eval_model, line, batch_size, candidates = candidates)\n",
    "            if batch_output != nothing\n",
    "                try\n",
    "                    write(output, batch_output)\n",
    "                catch e\n",
    "                    print(batch_output * \"\\n\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    close(output)\n",
    "    close(input)\n",
    "    return output_file\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"always_split_result2.en\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Vocabulary()\n",
    "input_file = \"test.en\"\n",
    "build_vocab(tokenizer, input_file)\n",
    "candidate_generator = PunctuationSpace()\n",
    "always_split_model = AlwaysSplitModel(tokenizer, 1, 1)\n",
    "eval_model = EvalModel(always_split_model)\n",
    "evaluate_model(eval_model, input_file, \"always_split_result2.en\", 16; candidates = candidate_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
